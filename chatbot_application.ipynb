{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4025a0a0",
   "metadata": {},
   "source": [
    "# Chatbot Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f7cff",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f2c26e",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cf8a6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ uv is already installed\n"
     ]
    }
   ],
   "source": [
    "# Install uv package manager if not already installed\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    subprocess.run(['uv', '--version'], capture_output=True, check=True)\n",
    "    print(\"✓ uv is already installed\")\n",
    "except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "    print(\"Installing uv...\")\n",
    "    %pip install uv\n",
    "    print(\"✓ uv installed successfully\")\n",
    "    print(\"Initializing uv...\")\n",
    "    %uv init .\n",
    "    print(\"✓ uv initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ae69767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n",
      "✓ PyTorch version: 2.9.1+cpu\n",
      "✓ CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Environment Detection and Setup\n",
    "try:\n",
    "    import google.colab # Check if running in Google Colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "    cache_dir = None  # Colab uses default HF cache\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")\n",
    "    # Use local models directory\n",
    "    import os\n",
    "    models_cache_dir = \"./models\"\n",
    "    os.makedirs(models_cache_dir, exist_ok=True)\n",
    "\n",
    "# Install dependencies with uv (works in both environments)\n",
    "!uv add gradio torch transformers accelerate bitsandbytes compressed-tensors --quiet\n",
    "\n",
    "import torch\n",
    "print(f\"✓ PyTorch version: {torch.__version__}\")\n",
    "print(f\"✓ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a4e66",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7822c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Koding dan Kecerdasan Artificial\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Gradio Interface\n",
    "import gradio as gr\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474b94e7",
   "metadata": {},
   "source": [
    "## Run the Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ee8eb5",
   "metadata": {},
   "source": [
    "### Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df50a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Koding dan Kecerdasan Artificial\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in E:\\Koding dan Kecerdasan Artificial\\models\\models--Qwen--Qwen2.5-0.5B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Using the same model you tried with vLLM\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"  # Qwen model\n",
    "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"  # Deepseek model\n",
    "\n",
    "# Set cache directory to local models folder\n",
    "models_cache_dir = \"./models\"\n",
    "\n",
    "print(\"Loading tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True,\n",
    "    cache_dir=models_cache_dir\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=models_cache_dir\n",
    ")\n",
    "\n",
    "print(\"✓ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f5e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic response generation function (without conversation history)\n",
    "def basic_response(prompt, max_tokens=512, temperature=0.7):\n",
    "    \"\"\"Generate a response from the model\n",
    "    \n",
    "    Usage:\n",
    "    prompt = \"Why is the sky blue?\"\n",
    "    response = generate_response(prompt)\n",
    "    print(f\"Question: {prompt}\")\n",
    "    print(f\"\\nAnswer: {response}\")\n",
    "    \"\"\"\n",
    "\n",
    "    # Format as chat message\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract just the assistant's response\n",
    "    response = generated_text.split(\"assistant\")[-1].strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "# Chat functionality with full conversation history\n",
    "def chat_with_model(message, history):\n",
    "    \"\"\"Chat with full conversation history\"\"\"\n",
    "    \n",
    "    # Build messages from history\n",
    "    messages = []\n",
    "    for user_msg, assistant_msg in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response.split(\"assistant\")[-1].strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cee8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Koding dan Kecerdasan Artificial\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Gradio interface\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat_with_model,\n",
    "    title=\"Chatbot Application\",\n",
    "    description=\"Chat with Qwen Model\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c002e",
   "metadata": {},
   "source": [
    "### llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9277407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install llama-cpp-python with GPU support using uv\n",
    "# This compiles with CUDA support for GPU acceleration\n",
    "!CMAKE_ARGS=\"-DGGML_CUDA=on\" uv pip install llama-cpp-python\n",
    "\n",
    "print(\"✓ llama-cpp-python installed with GPU support!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c848918c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ llama-cpp version: 0.3.16\n"
     ]
    }
   ],
   "source": [
    "# Verify installation\n",
    "try:\n",
    "    import llama_cpp\n",
    "    print(f\"✓ llama-cpp version: {llama_cpp.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"✗ Installation failed. Try running this cell again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c86ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Download a GGUF model\n",
    "# Using huggingface-cli to download models easily\n",
    "!uv pip install huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee672a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Koding dan Kecerdasan Artificial\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Qwen2.5-0.5B-Instruct-Q4_K_M.gguf from bartowski/Qwen2.5-0.5B-Instruct-GGUF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model downloaded to: ./models\\models--bartowski--Qwen2.5-0.5B-Instruct-GGUF\\snapshots\\41ba88dbac95fed2528c92514c131d73eb5a174b\\Qwen2.5-0.5B-Instruct-Q4_K_M.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Koding dan Kecerdasan Artificial\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in E:\\Koding dan Kecerdasan Artificial\\models\\models--bartowski--Qwen2.5-0.5B-Instruct-GGUF. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "# Example: Download a small quantized model\n",
    "# You can change this to any GGUF model from Hugging Face\n",
    "model_repo = \"bartowski/Qwen2.5-0.5B-Instruct-GGUF\"\n",
    "model_file = \"Qwen2.5-0.5B-Instruct-Q4_K_M.gguf\"  # Q4 quantization = good balance\n",
    "\n",
    "# Set cache directory to local models folder\n",
    "models_cache_dir = \"./models\"\n",
    "\n",
    "print(f\"Downloading {model_file} from {model_repo}...\")\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=model_repo,\n",
    "    filename=model_file,\n",
    "    cache_dir=models_cache_dir\n",
    ")\n",
    "\n",
    "print(f\"✓ Model downloaded to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f20891a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model into llama.cpp...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully!\n",
      "\n",
      "=== Test Response ===\n",
      " This is a very simple question, but one that can be quite challenging to answer. The sky is blue because of the scattering of sunlight by tiny particles in the air, such as water droplets and pollen. These particles absorb some of the light that enters the atmosphere, but some of it is reflected back into space. The remaining light is scattered in all directions, which we see as the sky. The blue color of the sky is due to the fact that the blue color is the color of the particles that absorb the most sunlight. The rest of the light is reflected back into space, which is why the sky appears blue. The color of the sky is also affected by the angle of the sun and the time of day. The sun's position in the sky and the time of day can affect the scattering of light and the color of the sky. Overall, the blue color of the sky is due to the scattering of sunlight by tiny particles in the air, which is caused by the fact that water droplets and pollen in the air absorb some of the light that enters the atmosphere.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Initialize llama.cpp and test\n",
    "from llama_cpp import Llama\n",
    "\n",
    "print(\"Loading model into llama.cpp...\")\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=4096,           # Context window size\n",
    "    n_gpu_layers=-1,      # -1 = offload all layers to GPU, 0 = CPU only\n",
    "    n_batch=512,          # Batch size for prompt processing\n",
    "    n_threads=4,          # Number of CPU threads\n",
    "    verbose=False         # Set to True for debugging\n",
    ")\n",
    "\n",
    "print(\"✓ Model loaded successfully!\")\n",
    "\n",
    "# Test generation\n",
    "response = llm(\n",
    "    \"Why is the sky blue?\",\n",
    "    max_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    stop=[\"User:\", \"\\n\\n\\n\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Test Response ===\")\n",
    "print(response['choices'][0]['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
